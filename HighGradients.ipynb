{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_pad(im, desired_shape = 128):\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "    ratio = float(desired_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "\n",
    "    # new_size should be in (width, height) format\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "\n",
    "    delta_w = desired_size - new_size[1]\n",
    "    delta_h = desired_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "        value=color)\n",
    "    return new_im\n",
    "def histequal(img):\n",
    "    img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "\n",
    "    # equalize the histogram of the Y channel\n",
    "    img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n",
    "\n",
    "    # convert the YUV image back to RGB format\n",
    "    img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
    "    \n",
    "    return img_output\n",
    "def histogram(img):\n",
    "    hist,bins = np.histogram(img.flatten(),256,[0,256])\n",
    "    cdf = hist.cumsum()\n",
    "    cdf_normalized = cdf * hist.max()/ cdf.max()\n",
    "    # plt.plot(cdf_normalized, color = 'g')\n",
    "    plt.hist(img.flatten(),256,[0,256], color = 'b')\n",
    "    plt.xlim([0,256])\n",
    "    plt.legend(('cdf','histogram'), loc = 'upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "train_data = np.load(\"Data/Train_data.npz\")\n",
    "train_data = train_data.f.arr_0\n",
    "train_labels = np.load(\"Data/Train_labels.npz\")\n",
    "train_labels = train_labels.f.arr_0\n",
    "val_data = np.load(\"Data/Validation_data.npz\")\n",
    "val_data = val_data.f.arr_0\n",
    "val_labels = np.load(\"Data/Validation_labels.npz\")\n",
    "val_labels = val_labels.f.arr_0\n",
    "test_data = np.load(\"Data/Test_data.npz\")\n",
    "test_data = test_data.f.arr_0\n",
    "test_labels = np.load(\"Data/Test_labels.npz\")\n",
    "test_labels = test_labels.f.arr_0\n",
    "\n",
    "val_data = val_data.reshape((val_data.shape[0], -1))\n",
    "test_data = test_data.reshape((test_data.shape[0], -1))\n",
    "m = train_data.shape[0]\n",
    "\n",
    "train_data = train_data.reshape((m, -1))\n",
    "\n",
    "print(\"Train Data : \" + str(train_data.shape))\n",
    "print(\"Train Labels : \" + str(train_labels.shape))\n",
    "\n",
    "print(\"Validation Data : \" + str(val_data.shape))\n",
    "print(\"Validation Labels : \" + str(val_labels.shape))\n",
    "\n",
    "print(\"Test Data : \" + str(test_data.shape))\n",
    "print(\"Test Labels : \" + str(test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.concatenate((train_data, val_data))\n",
    "train_labels = np.concatenate((train_labels, val_labels))\n",
    "print(\"Train Data : \" + str(train_data.shape))\n",
    "print(\"Train Labels : \" + str(train_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import urlopen\n",
    "\n",
    "def get_img(image_url):\n",
    "    # image_url = \"https://3.imimg.com/data3/QD/HN/MY-678214/helmet-250x250.jpg\"\n",
    "    # image_url = \"https://images-na.ssl-images-amazon.com/images/I/51Q0Ph9JmdL.jpg\"\n",
    "    f = open('test.jpg','wb')\n",
    "    f.write(urlopen(image_url).read())\n",
    "    f.close()\n",
    "\n",
    "    img = cv2.imread(\"test.jpg\")\n",
    "    imshow(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = resize_and_pad(img)\n",
    "    img = cv2.equalizeHist(img)\n",
    "    img = img / 255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=300)\n",
    "\n",
    "principalComponents = pca.fit_transform(train_data)\n",
    "print(principalComponents.shape)\n",
    "classifier = svm.SVC(kernel = 'rbf')\n",
    "classifier.fit(principalComponents, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = test_labels\n",
    "pc_test = pca.transform(test_data)\n",
    "print(pc_test.shape)\n",
    "predicted = classifier.predict(pc_test)\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "print(\"Accuracy : \" + str(accuracy_score(expected, predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 144)\n",
    "clf = clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=144, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        12\n",
      "          1       1.00      1.00      1.00        18\n",
      "          2       0.95      0.95      0.95        40\n",
      "          3       1.00      0.94      0.97        18\n",
      "          4       0.83      0.88      0.85        33\n",
      "          5       0.83      0.99      0.90        69\n",
      "          6       0.97      1.00      0.98        28\n",
      "          7       1.00      1.00      1.00        15\n",
      "          8       0.96      0.72      0.83        36\n",
      "          9       1.00      0.14      0.25         7\n",
      "         10       0.90      0.93      0.92        30\n",
      "         11       0.90      1.00      0.95        18\n",
      "\n",
      "avg / total       0.92      0.91      0.90       324\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 9  0  2  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 18  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 38  0  2  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 17  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0 29  2  0  0  0  0  2  0]\n",
      " [ 0  0  0  0  0 68  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0 28  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 15  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0 26  0  0  0]\n",
      " [ 0  0  0  0  4  2  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0 28  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 18]]\n",
      "Accuracy : 0.9104938271604939\n"
     ]
    }
   ],
   "source": [
    "expected = test_labels\n",
    "predicted = clf.predict(test_data)\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (clf, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "print(\"Accuracy : \" + str(accuracy_score(expected, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(clf, open(\"random_forest_model.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pickle.load(open(\"random_forest_model.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "param_grid = {\"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "             \"n_estimators\": [ 5, 10, 50, 100, 300],\n",
    "             \"learning_rate\": [0.1, 1]\n",
    "            }\n",
    "\n",
    "dectree = DecisionTreeClassifier(random_state = 11, max_features = \"auto\", max_depth = None)\n",
    "\n",
    "adb = AdaBoostClassifier(base_estimator = dectree)\n",
    "\n",
    "# run grid search\n",
    "gridsearchADB = GridSearchCV(adb, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "gridsearchADB.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = test_labels\n",
    "predicted = gridsearchADB.predict(test_data)\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (gridsearchADB, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "print(\"Accuracy : \" + str(accuracy_score(expected, predicted)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
